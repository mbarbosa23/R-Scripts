######## PACKAGES & SUCH ###########
if (!requireNamespace("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
BiocManager::install("dada2")


library(dada2)


###########################################################################
##########DADA2 PROCCESSING AND WORKFLOW
####Adapted from DADA2 pipeline tutorial: https://benjjneb.github.io/dada2/tutorial.html
#####IMPORT READS
path <- "/source" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)

#####BEGIN HERE TO EXAMINE QUALITY OF READS

# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
fnFs
# Inspect read quality profiles
plotQualityProfile(fnFs[1:6]) #Forward Reads

plotQualityProfile(fnRs[1:6]) #Reverse Reads
#?plotQualityProfile()

#####BEGIN HERE FOR DADA2 PROCESS
###Filter and Trim

# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names


#Filter forward and reverse reads. TruncLen is determined by inspecting quality profiles and is dependent on sequences.
#In this example, forward reads are truncated at bp 285 and reverse reads are truncated at bp 275.
#maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE and maxEE=2 are the default parameters.
#The maxEE parameter sets the maximum number of "expected errors" allowed in a read, which is a better filter than simply averaging quality scores.
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = 5,trimRight = 5, truncLen=c(285,275),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE, verbose=TRUE) # On Windows set multithread=FALSE
head(out)

#Determening Error Rates
errF <- learnErrors(filtFs, multithread=TRUE) #Forward Reads

errR <- learnErrors(filtRs, multithread=TRUE) #Reverse Reads

#Visualize estimated error rates
plotErrors(errF, nominalQ=TRUE)

plotErrors(errR, nominalQ=TRUE)

#Applying sample inference algorithm
dadaFs <- dada(filtFs, err=errF, multithread=TRUE, pool= TRUE) #Forward Reads

dadaRs <- dada(filtRs, err=errR, multithread=TRUE, pool= TRUE) #Reverse Reads

dadaFs[[1]] #Identifies amount of sequence variants from forward reads.

dadaRs[[1]] #Identifies amount of sequence variants from reverse reads.

###Merge paired reads

mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

###Construct sequence tables

seqtab <- makeSequenceTable(mergers) #Creates amplicon sequence table
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

##Optional: remove non-target-length sequences from your sequence table

#seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 400:550] #get amplicons of the targeted length


seqtab3 <- seqtab[,nchar(colnames(seqtab)) %in% 430:550] #get amplicons of the targeted length

###Remove chimeras

#seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE) #combines a left-segment and a right-segment from two more abundant "parent" sequences.
#dim(seqtab.nochim)

#sum(seqtab.nochim)/sum(seqtab2) #determines the amount of chimeras in merged sequence reads. 1-n = %chimeras.


###Track reads through the pipeline. Determine the number of reads that made it through each step in the pipeline

#getN <- function(x) sum(getUniques(x))
#track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)

#colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
#rownames(track) <- sample.names
#head(track)


###Remove chimeras with filtered table

seqtab3.nochim <- removeBimeraDenovo(seqtab3, method="consensus", multithread=TRUE, verbose=TRUE) #combines a left-segment and a right-segment from two more abundant "parent" sequences.
dim(seqtab3.nochim)

sum(seqtab3.nochim)/sum(seqtab3) #determines the amount of chimeras in merged sequence reads. 1-n = %chimeras.


###Track reads through the pipeline. Determine the number of reads that made it through each step in the pipeline

getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab3.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
track <- data.frame(track)
sum_reads <- sum(track$nonchim)
sum_reads


otu_table <- seqtab3.nochim
##Note: Outside of filtering, there should be no step in which a majority of reads are lost.
##If too many reads were lost return to filtering step.

#####END OF DADA2 PROCESSING

#####ASSIGN TAXONOMY TO GENERATED ASVs
###Bayesian Classifier
##Using PR2

#Assign taxonomy using trained classifier. Note: Takes very long so use with server or powerful computer.

#With PR2 4.14
Taxonomy_Table<- assignTaxonomy(seqtab3.nochim,"/media/HDD_4T/Max/LT_Protists/Protists/pr2_version_4.14.0_SSU_dada2.fasta.gz", 
                                    taxLevels = c("Kingdom","Supergroup","Division","Class","Order","Family","Genus","Species"),
                                    minBoot = 70,
                                    multithread=TRUE, verbose = TRUE)

tax <-Taxonomy_Table
#?assignTaxonomy
taxa.print <- Taxonomy_Table # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

Taxonomy_Table_Taxa <- as.data.frame(Taxonomy_Table)
write.csv(x = Taxonomy_Table_Taxa, "Taxonomy_Table_Taxa.csv")

OTU_Table <- as.data.frame(seqtab3.nochim)
write.csv(x = OTU_Table, "OTU_Table.csv")

# Flip table
seqtab.t <- as.data.frame(t(seqtab3.nochim))


# Pull out ASV repset
rep_set_ASVs <- as.data.frame(rownames(seqtab.t))
rep_set_ASVs <- mutate(rep_set_ASVs, ASV_ID = 1:n())
rep_set_ASVs$ASV_ID <- sub("^", "ASV_", rep_set_ASVs$ASV_ID)
rep_set_ASVs$ASV <- rep_set_ASVs$`rownames(seqtab.t)` 
rep_set_ASVs$`rownames(seqtab.t)` <- NULL

# Add ASV numbers to table
rownames(seqtab.t) <- rep_set_ASVs$ASV_ID

# Add ASV numbers to taxonomy
taxonomy <- as.data.frame(tax)
taxonomy$ASV <- as.factor(rownames(taxonomy))
taxonomy <- merge(rep_set_ASVs, taxonomy, by = "ASV")
rownames(taxonomy) <- taxonomy$ASV_ID
taxonomy_for_mctoolsr <- unite(taxonomy, "taxonomy", 
                               c("Kingdom","Supergroup","Division","Class","Order","Family","Genus","Species", "ASV_ID"),
                               sep = ";")

#Pull out only Eukaryotes ASVs
Euks_ASVs = taxonomy %>%
  filter(Kingdom =="Eukaryota") #%>% 
  #filter(Order != "Chloroplast")

# Arrange the taxonomy dataframe for writing
taxonomy_for_fasta <- Euks_ASVs %>%
  unite("TaxString", c("Kingdom","Supergroup","Division","Class","Order","Family","Genus","Species", "ASV_ID"), 
        sep = "_", remove = FALSE)

Euks_fas = Biostrings::DNAStringSet(taxonomy_for_fasta$ASV)
names(Euks_fas) = paste(taxonomy_for_fasta$TaxString)
head(Euks_fas)
Biostrings::writeXStringSet(Euks_fas, "Euks_fas.fasta")

#####END OF Taxonomic Assignment
